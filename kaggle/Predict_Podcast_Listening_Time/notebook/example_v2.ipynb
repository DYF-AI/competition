{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_optimized\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "CPU Count:          12\n",
      "Memory Avail:       51.65 GB / 57.48 GB (89.9%)\n",
      "Disk Space Avail:   3550.29 GB / 6519.49 GB (54.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-10 17:17:10,504\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\t\tContext path: \"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_optimized/ds_sub_fit/sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Running DyStack sub-fit ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Beginning AutoGluon training ... Time limit = 439s\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m AutoGluon will save models to \"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_optimized/ds_sub_fit/sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Train Data Rows:    533333\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Train Data Columns: 13\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Label Column:       Listening_Time_minutes\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Problem Type:       regression\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Preprocessing data ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Using Feature Generators to preprocess the data ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tAvailable Memory:                    51350.86 MB\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tTrain Data (Original)  Memory Usage: 52.90 MB (0.1% of available memory)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tStage 1 Generators:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tStage 2 Generators:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tStage 3 Generators:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tStage 4 Generators:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tStage 5 Generators:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tUseless Original Features (Count: 1): ['Has_Special']\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t('float', []) : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t('int', [])   : 8 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Episode_Sentiment', 'Time_Period', ...]\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t('float', [])     : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t('int', [])       : 7 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Episode_Sentiment', 'Time_Period', ...]\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t\t('int', ['bool']) : 1 | ['Is_Weekend']\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.7s = Fit runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t12 features in original data used to generate 12 features in processed data.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tTrain Data (Processed) Memory Usage: 45.27 MB (0.1% of available memory)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m User-specified model hyperparameters to be fit:\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m {\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t'GBM': [{'num_boost_round': 300}],\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t'CAT': [{'iterations': 1500}],\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t'XGB': [{'max_depth': 10}],\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m }\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 292.32s of the 438.58s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.62%)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-13.0894\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t17.03s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t9.12s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 270.89s of the 417.15s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.63%)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-13.073\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t195.92s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.44s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 71.94s of the 218.20s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.90%)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9614\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t57.72s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t40.81s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 150.51s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tEnsemble Weights: {'XGBoost_BAG_L1': 0.688, 'LightGBM_BAG_L1': 0.188, 'CatBoost_BAG_L1': 0.125}\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9329\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.21s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.0s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting 3 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 150.02s of the 149.88s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.70%)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9294\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t14.92s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t3.37s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 131.38s of the 131.23s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.71%)\n",
      "\u001B[36m(_ray_fit pid=85650)\u001B[0m \tRan out of time, early stopping on iteration 855.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9314\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t104.64s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.2s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 23.75s of the 23.60s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.01%)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9737\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t18.93s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t4.23s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 0.56s of remaining time.\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.286, 'CatBoost_BAG_L2': 0.286, 'XGBoost_BAG_L1': 0.214, 'XGBoost_BAG_L2': 0.143, 'LightGBM_BAG_L1': 0.071}\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t-12.9243\t = Validation score   (-root_mean_squared_error)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.45s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m \t0.0s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m AutoGluon training complete, total runtime = 439.96s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1145.9 rows/s (66667 batch size)\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_optimized/ds_sub_fit/sub_fit_ho\")\n",
      "\u001B[36m(_dystack pid=82685)\u001B[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       XGBoost_BAG_L1     -12.803423 -12.961389  root_mean_squared_error        6.576162      40.811569   57.716612                 6.576162               40.811569          57.716612            1       True          3\n",
      "1  WeightedEnsemble_L3     -12.821251 -12.924283  root_mean_squared_error       12.380778      58.182083  409.607945                 0.020928                0.004931           0.453315            3       True          8\n",
      "2       XGBoost_BAG_L2     -12.825767 -12.973681  root_mean_squared_error       11.185296      54.602165  289.593009                 1.413119                4.229243          18.932780            2       True          7\n",
      "3      LightGBM_BAG_L2     -12.827017 -12.929374  root_mean_squared_error       10.685930      53.743205  285.579689                 0.913754                3.370283          14.919460            2       True          5\n",
      "4      CatBoost_BAG_L2     -12.829668 -12.931412  root_mean_squared_error       10.032976      50.577626  375.302391                 0.260800                0.204704         104.642162            2       True          6\n",
      "5  WeightedEnsemble_L2     -12.833495 -12.932888  root_mean_squared_error        9.789334      50.377730  270.870835                 0.017158                0.004808           0.210606            2       True          4\n",
      "6      CatBoost_BAG_L1     -13.017049 -13.072982  root_mean_squared_error        0.891387       0.436530  195.916939                 0.891387                0.436530         195.916939            1       True          2\n",
      "7      LightGBM_BAG_L1     -13.042384 -13.089443  root_mean_squared_error        2.304627       9.124823   17.026678                 2.304627                9.124823          17.026678            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t466s\t = DyStack   runtime |\t1334s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1334s\n",
      "AutoGluon will save models to \"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_optimized\"\n",
      "Train Data Rows:    600000\n",
      "Train Data Columns: 13\n",
      "Label Column:       Listening_Time_minutes\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    50762.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Has_Special']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\t\t('int', [])   : 8 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Episode_Sentiment', 'Time_Period', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\t\t('int', [])       : 7 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Episode_Sentiment', 'Time_Period', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Is_Weekend']\n",
      "\t0.8s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.93 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.89s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'num_boost_round': 300}],\n",
      "\t'CAT': [{'iterations': 1500}],\n",
      "\t'XGB': [{'max_depth': 10}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 888.70s of the 1333.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.70%)\n",
      "\t-13.0843\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.68s\t = Training   runtime\n",
      "\t10.4s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 865.93s of the 1310.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.71%)\n",
      "\t-13.0662\t = Validation score   (-root_mean_squared_error)\n",
      "\t225.77s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 637.93s of the 1082.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.99%)\n",
      "\t-12.9416\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.94s\t = Training   runtime\n",
      "\t50.5s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 983.21s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.714, 'LightGBM_BAG_L1': 0.19, 'CatBoost_BAG_L1': 0.095}\n",
      "\t-12.9169\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 3 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 982.80s of the 982.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.79%)\n",
      "\t-12.9075\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.43s\t = Training   runtime\n",
      "\t4.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 962.51s of the 962.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.80%)\n",
      "\t-12.9098\t = Validation score   (-root_mean_squared_error)\n",
      "\t170.99s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 789.23s of the 789.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.13%)\n",
      "\t-12.9421\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.53s\t = Training   runtime\n",
      "\t4.51s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 763.34s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.4, 'CatBoost_BAG_L2': 0.3, 'XGBoost_BAG_L2': 0.2, 'XGBoost_BAG_L1': 0.1}\n",
      "\t-12.9026\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 572.21s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1067.4 rows/s (75000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_optimized\")\n"
     ]
    }
   ],
   "source": [
    "import autogluon\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def add_features(df):\n",
    "    # 日期特征\n",
    "    day_map = {'Monday':0, 'Tuesday':1, 'Wednesday':2, 'Thursday':3,\n",
    "             'Friday':4, 'Saturday':5, 'Sunday':6}\n",
    "    df['Publication_Day'] = df['Publication_Day'].map(day_map)\n",
    "\n",
    "    # 时间分类特征处理\n",
    "    time_map = {\"Morning\":7, \"Afternoon\":8, \"Evening\":9, \"Night\":10}\n",
    "    df['Time_Period'] = df['Publication_Time'].map(time_map)\n",
    "\n",
    "    # 新增交互特征\n",
    "    df['Is_Weekend'] = (df['Publication_Day'] >= 5).astype(int)\n",
    "    df['Genre_Time'] = df['Genre'] + '_' + df['Time_Period'].astype(str)\n",
    "\n",
    "    # 文本特征处理\n",
    "    df['Title_Length'] = df['Episode_Title'].str.len()\n",
    "    df['Has_Special'] = df['Episode_Title'].str.contains('特别版|直播|专访').astype(int)\n",
    "\n",
    "    # 分类特征编码\n",
    "    cat_features = ['Podcast_Name', 'Genre', 'Episode_Sentiment', 'Genre_Time']\n",
    "    for col in cat_features:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    # 删除冗余列\n",
    "    return df.drop(columns=['Publication_Time', 'Episode_Title'], errors='ignore')\n",
    "\n",
    "# 数据加载与处理\n",
    "train = pd.read_csv(\"../data/playground-series-s5e4/train.csv\", index_col='id')\n",
    "test = pd.read_csv('../data/playground-series-s5e4/test.csv', index_col='id')\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 模型训练配置优化\n",
    "predictor = TabularPredictor(\n",
    "    label='Listening_Time_minutes',\n",
    "    problem_type='regression',\n",
    "    eval_metric='root_mean_squared_error',\n",
    "    path='ag_optimized'\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    #tuning_data=val_data,\n",
    "    presets='best_quality',\n",
    "    time_limit=1800,\n",
    "    hyperparameters={\n",
    "        'GBM': {'num_boost_round': 300},\n",
    "        'CAT': {'iterations': 1500},\n",
    "        'XGB': {'max_depth': 10}\n",
    "    },\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# 生成预测结果\n",
    "test_pred = predictor.predict(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "id\n750000    55.327278\n750001    18.398293\n750002    50.291405\n750003    74.953148\n750004    46.769905\n            ...    \n999995    11.029882\n999996    58.795971\n999997     6.645740\n999998    74.079834\n999999    57.524734\nName: Listening_Time_minutes, Length: 250000, dtype: float32"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#submission = pd.DataFrame({'id': test.index, 'Listening_Time_minutes': test_pred})\n",
    "submission = pd.DataFrame(test_pred)\n",
    "submission.to_csv('../output/submission.csv') # 12.90"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
