{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "train_df = pd.read_csv(\"../data/playground-series-s5e4/train.csv\", index_col='id')\n",
    "test_df = pd.read_csv('../data/playground-series-s5e4/test.csv', index_col='id')\n",
    "submission_df = pd.read_csv('../data/playground-series-s5e4/sample_submission.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# # Feature engineering function\n",
    "# ref：https://www.kaggle.com/code/greysky/ps-s5e4-lgbm-cv-12-25-lb-12-15\n",
    "# def add_features(df):\n",
    "#\n",
    "#     # Convert binary features to numeric\n",
    "#     day_map = {\n",
    "#         'Monday': 0,\n",
    "#         'Tuesday': 1,\n",
    "#         'Wednesday': 2,\n",
    "#         'Thursday': 3,\n",
    "#         'Friday': 4,\n",
    "#         'Saturday': 5,\n",
    "#         'Sunday': 6,\n",
    "#     }\n",
    "#     df['Publication_Day'] = df['Publication_Day'].map(day_map)\n",
    "#\n",
    "#     time_map = {\n",
    "#        \"Morning\": 0,\n",
    "#        \"Afternoon\": 1,\n",
    "#        \"Night\": 2\n",
    "#    }\n",
    "#\n",
    "#     df['Publication_Time'] = df['Publication_Time'].map(time_map)\n",
    "#\n",
    "#     return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    podc_dict = {'Mystery Matters': 0, 'Joke Junction': 1, 'Study Sessions': 2, 'Digital Digest': 3, 'Mind & Body': 4, 'Fitness First': 5, 'Criminal Minds': 6, 'News Roundup': 7, 'Daily Digest': 8, 'Music Matters': 9, 'Sports Central': 10, 'Melody Mix': 11, 'Game Day': 12, 'Gadget Geek': 13, 'Global News': 14, 'Tech Talks': 15, 'Sport Spot': 16, 'Funny Folks': 17, 'Sports Weekly': 18, 'Business Briefs': 19, 'Tech Trends': 20, 'Innovators': 21, 'Health Hour': 22, 'Comedy Corner': 23, 'Sound Waves': 24, 'Brain Boost': 25, \"Athlete's Arena\": 26, 'Wellness Wave': 27, 'Style Guide': 28, 'World Watch': 29, 'Humor Hub': 30, 'Money Matters': 31, 'Healthy Living': 32, 'Home & Living': 33, 'Educational Nuggets': 34, 'Market Masters': 35, 'Learning Lab': 36, 'Lifestyle Lounge': 37, 'Crime Chronicles': 38, 'Detective Diaries': 39, 'Life Lessons': 40, 'Current Affairs': 41, 'Finance Focus': 42, 'Laugh Line': 43, 'True Crime Stories': 44, 'Business Insights': 45, 'Fashion Forward': 46, 'Tune Time': 47}\n",
    "    genr_dict = {'True Crime': 0, 'Comedy': 1, 'Education': 2, 'Technology': 3, 'Health': 4, 'News': 5, 'Music': 6, 'Sports': 7, 'Business': 8, 'Lifestyle': 9}\n",
    "    week_dict = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "    time_dict = {'Morning': 0, 'Afternoon': 1, 'Evening': 2, 'Night': 3}\n",
    "    sent_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "    df['Episode_Num'] = df['Episode_Title'].str[8:].astype('category')\n",
    "\n",
    "    df['Genre'] = df['Genre'].replace(genr_dict)\n",
    "    df['Podcast_Name'] = df['Podcast_Name'].replace(podc_dict)\n",
    "    df['Publication_Day'] = df['Publication_Day'].replace(week_dict)\n",
    "    df['Publication_Time'] = df['Publication_Time'].replace(time_dict)\n",
    "    df['Episode_Sentiment'] = df['Episode_Sentiment'].replace(sent_dict)\n",
    "\n",
    "    df['Genre'] = df['Genre'].astype('category')\n",
    "    df['Podcast_Name'] = df['Podcast_Name'].astype('category')\n",
    "    df['Publication_Day'] = df['Publication_Day'].astype('category')\n",
    "    df['Publication_Time'] = df['Publication_Time'].astype('category')\n",
    "    df['Episode_Sentiment'] = df['Episode_Sentiment'].astype('category')\n",
    "\n",
    "    df = df.drop(columns=['Episode_Title'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123312/2953749971.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Genre'] = df['Genre'].replace(genr_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Podcast_Name'] = df['Podcast_Name'].replace(podc_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Publication_Day'] = df['Publication_Day'].replace(week_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Publication_Time'] = df['Publication_Time'].replace(time_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Episode_Sentiment'] = df['Episode_Sentiment'].replace(sent_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Genre'] = df['Genre'].replace(genr_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Podcast_Name'] = df['Podcast_Name'].replace(podc_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Publication_Day'] = df['Publication_Day'].replace(week_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Publication_Time'] = df['Publication_Time'].replace(time_dict)\n",
      "/tmp/ipykernel_123312/2953749971.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Episode_Sentiment'] = df['Episode_Sentiment'].replace(sent_dict)\n"
     ]
    }
   ],
   "source": [
    "# #Apply feature engineering to all datasets\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:40<00:00,  4.78s/it]\n",
      "100%|██████████| 35/35 [03:21<00:00,  5.74s/it]\n",
      "100%|██████████| 35/35 [03:54<00:00,  6.70s/it]\n"
     ]
    }
   ],
   "source": [
    "encode_columns = ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Publication_Day', 'Publication_Time']\n",
    "pair_size = [2, 3, 4]\n",
    "\n",
    "for r in pair_size:\n",
    "    for cols in tqdm(list(combinations(encode_columns, r))):\n",
    "        new_col_name = '_'.join(cols)\n",
    "\n",
    "        train_df[new_col_name] = train_df[list(cols)].astype(str).agg('_'.join, axis=1)\n",
    "        train_df[new_col_name] = train_df[new_col_name].astype('category')\n",
    "\n",
    "        test_df[new_col_name] = test_df[list(cols)].astype(str).agg('_'.join, axis=1)\n",
    "        test_df[new_col_name] = test_df[new_col_name].astype('category')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "encoded_columns = train_df.columns[11:]\n",
    "encoder = TargetEncoder(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define target column\n",
    "target = 'Listening_Time_minutes'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "       Podcast_Name  Episode_Length_minutes Genre  Host_Popularity_percentage  \\\n453635           44                   94.30     0                       97.67   \n11651            36                   82.15     2                       94.78   \n431999           20                   13.72     3                       68.60   \n529211           43                   24.00     1                       42.14   \n110925           26                     NaN     7                       34.10   \n...             ...                     ...   ...                         ...   \n259178           38                   42.65     0                       80.53   \n365838           45                   94.50     8                       42.80   \n131932           11                   61.54     6                       63.97   \n671155           19                   87.86     8                       75.76   \n121958           22                     NaN     4                       68.21   \n\n       Publication_Day Publication_Time  Guest_Popularity_percentage  \\\n453635               4                1                          NaN   \n11651                5                3                          NaN   \n431999               5                1                        65.77   \n529211               5                0                        41.29   \n110925               4                3                          NaN   \n...                ...              ...                          ...   \n259178               4                1                        96.31   \n365838               4                1                         8.68   \n131932               3                1                        62.82   \n671155               2                3                        74.87   \n121958               1                2                          NaN   \n\n        Number_of_Ads Episode_Sentiment  Listening_Time_minutes Episode_Num  \n453635            2.0                 2                77.27788          81  \n11651             1.0                 2                50.02839          53  \n431999            3.0                 0                10.07496          21  \n529211            0.0                 0                17.82074          99  \n110925            0.0                 1                94.80341          15  \n...               ...               ...                     ...         ...  \n259178            3.0                 0                16.77295          71  \n365838            3.0                 2                73.00649          66  \n131932            2.0                 0                38.48631          18  \n671155            0.0                 2                69.27837          27  \n121958            0.0                 0                86.21829          99  \n\n[600000 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Podcast_Name</th>\n      <th>Episode_Length_minutes</th>\n      <th>Genre</th>\n      <th>Host_Popularity_percentage</th>\n      <th>Publication_Day</th>\n      <th>Publication_Time</th>\n      <th>Guest_Popularity_percentage</th>\n      <th>Number_of_Ads</th>\n      <th>Episode_Sentiment</th>\n      <th>Listening_Time_minutes</th>\n      <th>Episode_Num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>453635</th>\n      <td>44</td>\n      <td>94.30</td>\n      <td>0</td>\n      <td>97.67</td>\n      <td>4</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>77.27788</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>11651</th>\n      <td>36</td>\n      <td>82.15</td>\n      <td>2</td>\n      <td>94.78</td>\n      <td>5</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>50.02839</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>431999</th>\n      <td>20</td>\n      <td>13.72</td>\n      <td>3</td>\n      <td>68.60</td>\n      <td>5</td>\n      <td>1</td>\n      <td>65.77</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>10.07496</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>529211</th>\n      <td>43</td>\n      <td>24.00</td>\n      <td>1</td>\n      <td>42.14</td>\n      <td>5</td>\n      <td>0</td>\n      <td>41.29</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>17.82074</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>110925</th>\n      <td>26</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>34.10</td>\n      <td>4</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>94.80341</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>259178</th>\n      <td>38</td>\n      <td>42.65</td>\n      <td>0</td>\n      <td>80.53</td>\n      <td>4</td>\n      <td>1</td>\n      <td>96.31</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>16.77295</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>365838</th>\n      <td>45</td>\n      <td>94.50</td>\n      <td>8</td>\n      <td>42.80</td>\n      <td>4</td>\n      <td>1</td>\n      <td>8.68</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>73.00649</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>131932</th>\n      <td>11</td>\n      <td>61.54</td>\n      <td>6</td>\n      <td>63.97</td>\n      <td>3</td>\n      <td>1</td>\n      <td>62.82</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>38.48631</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>671155</th>\n      <td>19</td>\n      <td>87.86</td>\n      <td>8</td>\n      <td>75.76</td>\n      <td>2</td>\n      <td>3</td>\n      <td>74.87</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>69.27837</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>121958</th>\n      <td>22</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>68.21</td>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>86.21829</td>\n      <td>99</td>\n    </tr>\n  </tbody>\n</table>\n<p>600000 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models_backpack\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "CPU Count:          12\n",
      "Memory Avail:       51.36 GB / 57.48 GB (89.4%)\n",
      "Disk Space Avail:   3550.26 GB / 6519.49 GB (54.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 1200s\n",
      "AutoGluon will save models to \"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_models_backpack\"\n",
      "Train Data Rows:    600000\n",
      "Train Data Columns: 10\n",
      "Tuning Data Rows:    150000\n",
      "Tuning Data Columns: 10\n",
      "Label Column:       Listening_Time_minutes\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    52533.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 27.19 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 6 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment', ...]\n",
      "\t\t('float', [])    : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 6 | ['Podcast_Name', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment', ...]\n",
      "\t\t('float', [])    : 4 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "\t1.2s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.36s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['FASTAI', 'NN_TORCH', 'KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 7 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 1198.64s of the 1198.64s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 12.987\n",
      "[2000]\tvalid_set's rmse: 12.9599\n",
      "[3000]\tvalid_set's rmse: 12.9493\n",
      "[4000]\tvalid_set's rmse: 12.9455\n",
      "[5000]\tvalid_set's rmse: 12.9464\n",
      "[6000]\tvalid_set's rmse: 12.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.944\t = Validation score   (-root_mean_squared_error)\n",
      "\t64.18s\t = Training   runtime\n",
      "\t5.31s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1128.88s of the 1128.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 12.9368\n",
      "[2000]\tvalid_set's rmse: 12.9136\n",
      "[3000]\tvalid_set's rmse: 12.9037\n",
      "[4000]\tvalid_set's rmse: 12.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.8991\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.62s\t = Training   runtime\n",
      "\t3.37s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 1088.68s of the 1088.68s of remaining time.\n",
      "\t-12.8035\t = Validation score   (-root_mean_squared_error)\n",
      "\t133.33s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 949.54s of the 949.53s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5177.\n",
      "\t-12.9383\t = Validation score   (-root_mean_squared_error)\n",
      "\t952.68s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -5.09s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE': 0.579, 'LightGBM': 0.263, 'LightGBMXT': 0.158}\n",
      "\t-12.7292\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1205.81s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15912.8 rows/s (150000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/mnt/n/code/competition/kaggle/Predict_Podcast_Listening_Time/notebook/ag_models_backpack\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Initialize AutoGluon predictor with time constraints\n",
    "predictor = TabularPredictor(\n",
    "    label=target,\n",
    "    problem_type='regression',\n",
    "    eval_metric='root_mean_squared_error',\n",
    "    path='ag_models_backpack'\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    tuning_data=val_data,\n",
    "    # Use medium_quality preset instead of best_quality for faster training\n",
    "    presets='medium_quality',\n",
    "    # Set a strict 10-minute time limit (600 seconds)\n",
    "    time_limit=600,\n",
    "    # Skip hyperparameter tuning to save time\n",
    "    hyperparameters='default',\n",
    "    # Limit model types to faster ones\n",
    "    excluded_model_types=['KNN', 'NN_TORCH', 'FASTAI'],\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# # Evaluate on validation data\n",
    "# performance = predictor.evaluate(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'root_mean_squared_error': -12.729220485024687,\n 'mean_squared_error': -162.03305415637212,\n 'mean_absolute_error': -9.267427073898412,\n 'r2': 0.7797947340895919,\n 'pearsonr': 0.883072301499459,\n 'median_absolute_error': -6.74962187866211}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(id\n 750000    55.539909\n 750001    18.064295\n 750002    49.844246\n 750003    76.765488\n 750004    47.959488\n             ...    \n 999995    11.845755\n 999996    57.767044\n 999997     7.463294\n 999998    73.461227\n 999999    57.888245\n Name: Listening_Time_minutes, Length: 250000, dtype: float32,\n 404846    31.373402\n 580313    34.161953\n 552086    36.372780\n 370876    46.505463\n 239330    46.874146\n             ...    \n 235496    53.114147\n 372040    35.043919\n 695665     2.838228\n 386092    81.920715\n 549832    78.157013\n Name: Listening_Time_minutes, Length: 150000, dtype: float32)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Generate predictions on test data\n",
    "# test_pred = predictor.predict(test_df)\n",
    "# val_pred = predictor.predict(val_data)\n",
    "# test_pred, val_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({'id': test.index, 'Listening_Time_minutes': test_pred})\n",
    "submission.set_index('id', inplace=True)\n",
    "submission.to_csv('../output/submission.csv')  # 12.81 SOTA\n",
    "print(\"Submission file created\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
